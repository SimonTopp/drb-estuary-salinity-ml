munge_usgs.py:
  # choose where you want to read your data inputs from: local or S3
  read_location: 'local'
  # choose where you want to write your data outputs: local or S3
  write_location: 'local'
  # set name of AWS profile storing credentials for S3
  aws_profile: 'dev'
  # set AWS bucket to read/write to
  s3_bucket: 'drb-estuary-salinity'

  # determine which data flags we want to drop  
  flags_to_drop:
    - 'e' # Value has been edited or estimated by USGS personnel and is write protected
    - '&' # Value was computed from affected unit values
    - 'E' # Value was computed from estimated unit values.
    # - 'A' # Approved for publication -- Processing and review completed.
    - 'P' # Provisional data subject to revision.
    - '<' # The value is known to be less than reported value and is write protected.
    - '>' # The value is known to be greater than reported value and is write protected.
    - '1' # Value is write protected without any remark code to be printed
    - '2' # Remark is write protected without any remark code to be printed
    
  # number of measurements required to consider average valid
  # we will assume that we need half of the timestep measurements
  prop_obs_required: 0.05

  # timestep to aggregate to
  # options come from data offsets found here:
  # https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html
  agg_level: 'D'

  # list of parameters to keep in processed data
  params_to_process:
    - '00060' # Discharge, cubic feet per second
    - '00065' # Gage height, feet
    - '00095' # Specific conductance, water, unfiltered, microsiemens per centimeter at 25 degrees Celsius
    - '00010' # Temperature, water, degrees Celsius
    - '72279' # Tidal elevation, NOS-averaged, NAVD88, feet
    - '63680' # Turbidity, water, unfiltered, monochrome near infra-red LED light, 780-900 nm, detection angle 90 +-2.5 degrees, formazin nephelometric units (FNU)
    - '00400' # pH, water, unfiltered, field, standard units

munge_noaa_nos.py:
  # choose where you want to read your data inputs from: local or S3
  read_location: 'local'
  # choose where you want to write your data outputs: local or S3
  write_location: 'local'
  # set name of AWS profile storing credentials for S3
  aws_profile: 'dev'
  # set AWS bucket to read/write to
  s3_bucket: 'drb-estuary-salinity'

  # determine which data flags we want to drop, by variable
  # flags marked True will be dropped
  flags_to_drop:
    # (verified) Water Level Data Flags - in order of listing:
    # (I) A flag that when set to 1 indicates that the water level value has been inferred
    # (F) A flag that when set to 1 indicates that the flat tolerance limit was exceeded
    # (R) A flag that when set to 1 indicates that the rate of change tolerance limit was exceeded
    # (T) A flag that when set to 1 indicates that either the maximum or minimum expected water level height limit was exceeded
    'water_level' : [False, True, True, True]
    # Conductivity Data Flags - in order of listing:
    # (X) A flag that when set to 1 indicates that the maximum expected conductivity was exceeded
    # (N) A flag that when set to 1 indicates that the minimum expected conductivity was exceeded
    # (R) A flag that when set to 1 indicates that the rate of change tolerance limit was exceeded
    'conductivity': [True, True, True]
  
  # determine what QA/QC levels to drop
  qa_to_drop:
    - 'p' # preliminary
    # - 'v' # verified

  # number of measurements required to consider average valid
  # we will assume that we need half of the timestep measurements
  prop_obs_required: 0.05

  # timestep to aggregate to
  # options come from data offsets found here:
  # https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html
  agg_level: 'H'
  
  butterworth_filter_params:
    #filter order
    order_butter: 6
    # Duration to capture tide signal (hrs)
    Cf: 32
    # cutoff frequency
    fc: 0.015625 # equal to (1/Cf)/2
    # sample interval
    fs: 10
    # desired product to filter
    product: 'water_level'

fill_discharge_prms.py
    #location of data release
    sb_url: 'https://www.sciencebase.gov/catalog/file/get/5f6a289982ce38aaa2449135?f=__disk__a6%2Fb3%2Ffb%2Fa6b3fb14393cb6b83d25897b395a92c4f4a193ed'
    #data file
    prms_predictions: 'sntemp_inputs_outputs_drb'
    #destination to write 
    destination: '99_scratch/drb_estuary_salinity_scratch/data/'
    #segment ids for trenton and schuylkill
    trenton_seg_id_nat: 1498
    schuylkill_seg_id_nat: 2338
    #location of the files in munge, they will be overwritten with gap filled data
    nwis_trenton_location: '02_munge/out/usgs_nwis_01463500.csv'
    nwis_schuylkill_location: '02_munge/out/usgs_nwis_01474500.csv'
    

