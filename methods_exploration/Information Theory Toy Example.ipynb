{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc161951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import HBox, interact, interactive, IntSlider, Label \n",
    "from IPython.display import display\n",
    "import it_functions\n",
    "import it_functions_examples as ife\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib widget\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [12, 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705cc46c",
   "metadata": {},
   "source": [
    "# Application of information theory to synthetic data\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34465d5d",
   "metadata": {},
   "source": [
    "This notebook is a demonstration of the calculation and visualization of mutual information and transfer entropy using synthetic data with known time lags of interaction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615897bd",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32982535",
   "metadata": {},
   "source": [
    ">*mutual information* - the amount of information obtained by one variable when observing another. Here mutual information between source (x) and sink (y) is normalized by the shannon entropy of the sink variables, so that the output can be conceptualized as the fraction of uncertainty in the sink variable that can be explained by the source variable. Mutual information is as symmetric metric ($MI_{xy} == MI_{yx}$) and is similar to a non-linear correlation __[More info](https://github.com/pdirmeyer/l-a-cheat-sheets/blob/main/Coupling_metrics_V30_MI.pdf)__\n",
    "\n",
    ">*transfer entropy* - the reduction of uncertainty in the sink variable (y) due to knowledge of the source variable (x) independent of reduction in uncertainty in the sink variable due to knowledge of its own past.  The output can be conceptualized as the fraction of entropy in the sink variable that can be explained by the source variable at a certain time lag, in excess of the sink variable's own history. Transfer entropy is an asymmetrical metric ($TE_{x->y} \\neq TE_{y->x}$) __[More info](https://github.com/pdirmeyer/l-a-cheat-sheets/blob/main/Coupling_metrics_V30_TE.pdf)__\n",
    "\n",
    ">For both metrics, significance is determined by shuffling the time series to destroy temporal relationships and calculating mutual information and transfer entropy. This is done 1000 times and we use the 99th percentile of those metrics calculated on shuffled values as a significance threshold for each metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed550ef2",
   "metadata": {},
   "source": [
    "## Logistic mapping with known time lag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7ea511",
   "metadata": {},
   "source": [
    "> Using a choatic logistic mapping with a known time lag, we can see how effective mutual information and transfer entropy are at identifying that time lag. The logistic map is defined as:\n",
    "\n",
    "> $y_t = ax_{(t-lag)} [1-x_{(t-lag)}]+z_te$\n",
    "\n",
    "> with lag = 5, a = 4 as the growth rate, z is drawn from a random guassian distribution, and e = 0.2 is a noise factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64033c3",
   "metadata": {},
   "source": [
    "### Mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c21f53ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ccfb0049e648969a12b056f911b0e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntText(value=500, description='Number of data points:'), FloatText(value=0.2, descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f(ndata, e):\n",
    "    ife.gen_plot_logistic_it_mi(ndata, e)\n",
    "interactive_plot = interactive(f,\n",
    "                                ndata = widgets.IntText(value=500,description='Number of data points:',disabled=False),\n",
    "                                e = widgets.FloatText(value = 0.2, description = 'e:'))\n",
    "                               \n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c8b90b",
   "metadata": {},
   "source": [
    "> The plot above shows the mutual information (blue), critical mutual information (blue dashed), and the pearson correlation coefficient (red) all calculated across a range of time lags. A couple of things to note:\n",
    "> - Mutual information shows a significant value only at the correct time lag (5), at all other time lags the mutual information is below the significance value\n",
    "> - Pearson correlation coefficient does show a peak there, but it's difficult to identify this as the most important time lag\n",
    "> - By adjusting the two interactive inputs at the top of the plot (number of data points and random noise), you can see how those affect the mutual information calculations. Fewer data points and more noise makes the singal less clear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3820b2e0",
   "metadata": {},
   "source": [
    "### Transfer entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9102d9f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628e6780d1c74eba8c98d776b9cff1e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntText(value=500, description='Number of data points:'), FloatText(value=0.2, descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f(ndata, e):\n",
    "    ife.gen_plot_logistic_it_te(ndata, e)\n",
    "interactive_plot = interactive(f,\n",
    "                                ndata = widgets.IntText(value=500,description='Number of data points:',disabled=False),\n",
    "                                e = widgets.FloatText(value = 0.2, description = 'Random noise:'))\n",
    "                               \n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2390edb",
   "metadata": {},
   "source": [
    "> The plot above shows the transfer entropy from source to sink (blue) and from sink to source (orange), as well as the respective critical values for transfer entropy in both directions (dotted lines). A couple things to note:\n",
    "> - With number of data = 500 and random noise = 0.2, the known critical time lag (5) should be the only significant transfer entropy value \n",
    "> - No value for $TE_{y->x}$ should rise above the critical value, unless the number of data is very low or the noise is very high\n",
    "> - Similar to mutual information, by adjusting the two interactive inputs at the top of the plot (number of data points and random noise), you can see how those affect the mutual information calculations. Fewer data points and more noise makes the singal less clear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6f6878",
   "metadata": {},
   "source": [
    "## Periodic signal with variable coupling coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097256a3",
   "metadata": {},
   "source": [
    "> Here we use a different relationship between x and y with coupling at multiple different time lags. The coupling also takes on multiple different functional forms:\n",
    "\n",
    "> $y_t = cc_1 e^{x_{t-1}}+cc_2{x_{x-2}}^2+cc_3x_{x-3}+cc_4cos(x_{x-4})+z_te$\n",
    "\n",
    "> where $cc_{1,2,3,4}$ are coupling coefficients and z is drawn from a random guassian distribution, and e = 0.2 is a noise factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bba5a77",
   "metadata": {},
   "source": [
    "### Mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86c3d535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3d968997df4aafac118c84c327cb7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntText(value=1000, description='Number of data points:'), FloatText(value=2.0, descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f(ndata, cc1, cc2, cc3, cc4, e):\n",
    "    ife.gen_plot_periodic_it_mi(ndata, cc1, cc2, cc3, cc4, e)\n",
    "interactive_plot = interactive(f,\n",
    "                                ndata = widgets.IntText(value=500,description='Number of data points:',disabled=False),\n",
    "                               cc1 = widgets.FloatText(value = 2, description = '$cc_1$: '),\n",
    "                               cc2 = widgets.FloatText(value = 7, description = \"$cc_2$ :\"),\n",
    "                               cc3 = widgets.FloatText(value = 0.5, description = \"$cc_3 :$\"),\n",
    "                               cc4 = widgets.FloatText(value = 5, description = \"$cc_4$ :\"),                               \n",
    "                               e = widgets.FloatText(value = 0.2, description = 'e :'))\n",
    "                               \n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d019ca",
   "metadata": {},
   "source": [
    "> The plot above shows the mutual information (blue), critical mutual information (blue dashed), and the pearson correlation coefficient (red) all calculated across a range of time lags. A couple of things to note:\n",
    "> - With default values mutual information shows a significant value only at time lags of 1, 2, and 4, but not 3\n",
    "> - Pearson correlation coefficient does show high values at 1, 2, and 3\n",
    "> - By adjusting the six interactive inputs at the top of the plot (number of data points, coupling coefficients, and random noise), you can see how those affect the mutual information calculations. Fewer data points, weaker coupling, and more noise makes the singal less clear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4476c0e",
   "metadata": {},
   "source": [
    "### Transfer entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebde5528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ad8b663b9d4fcc886d6f97eaf30a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntText(value=500, description='Number of data points:'), FloatText(value=2.0, descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f(ndata, cc1, cc2, cc3, cc4, e):\n",
    "    ife.gen_plot_periodic_it_te(ndata, cc1, cc2, cc3, cc4, e)\n",
    "interactive_plot = interactive(f,\n",
    "                                ndata = widgets.IntText(value=500,description='Number of data points:',disabled=False),\n",
    "                               cc1 = widgets.FloatText(value = 2, description = '$cc_1$: '),\n",
    "                               cc2 = widgets.FloatText(value = 7, description = \"$cc_2$ :\"),\n",
    "                               cc3 = widgets.FloatText(value = 0.5, description = \"$cc_3 :$\"),\n",
    "                               cc4 = widgets.FloatText(value = 5, description = \"$cc_4$ :\"),                               \n",
    "                               e = widgets.FloatText(value = 0.2, description = 'e :'))\n",
    "                               \n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6859c6a4",
   "metadata": {},
   "source": [
    "> The plot above shows the transfer entropy from source to sink (blue) and from sink to source (orange), as well as the respective critical values for transfer entropy in both directions (dotted lines). A couple things to note:\n",
    "> - With number of data = 500 and random noise = 0.2, the cofficients set to their default values, then a time lag of 2 should be the only significant transfer entropy value \n",
    "> - No value for $TE_{y->x}$ should rise above the critical value, unless the number of data is very low or the noise is very high\n",
    "> - Similar to mutual information, by adjusting the six interactive inputs at the top of the plot (number of data points, coupling coefficients, and random noise), you can see how those affect the mutual information calculations. Fewer data points, weaker coupling, and more noise makes the singal less clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4659e33c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
